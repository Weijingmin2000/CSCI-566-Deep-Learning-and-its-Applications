class BatchNorm2D(object):
    def __init__(self, num_features, eps=1e-05, momentum=0.9, name="batchnorm"):
        self.name = name
        self.eps = eps
        self.momentum = momentum
        self.num_features = num_features
        
        # Initialize parameters and gradients
        self.params = {}
        self.grads = {}
        self.params[self.name + '_gamma'] = np.ones(num_features)
        self.params[self.name + '_beta'] = np.zeros(num_features)
        self.grads[self.name + '_gamma'] = np.zeros(num_features)
        self.grads[self.name + '_beta'] = np.zeros(num_features)
        
        # Initialize running mean and variance
        self.params[self.name + '_running_mean'] = np.zeros(num_features)
        self.params[self.name + '_running_var'] = np.zeros(num_features)
        # self.running_mean = np.zeros(num_features)
        # self.running_var = np.zeros(num_features)
        
    def forward(self, x, train=True):
        N, C, H, W = x.shape
        
        # Compute sample mean and variance
        if train:
            sample_mean = np.mean(x, axis=(0, 2, 3))
            sample_var = np.var(x, axis=(0, 2, 3))
            
            # Update running mean and variance using momentum
            self.params[self.name + '_running_mean'] = self.momentum *  self.params[self.name + '_running_mean'] + (1 - self.momentum) * sample_mean
            self.params[self.name + '_running_var'] = self.momentum * self.params[self.name + '_running_var'] + (1 - self.momentum) * sample_var
            
        else:
            # Use running mean and variance during inference
            sample_mean = self.params[self.name + '_running_mean']
            sample_var = self.params[self.name + '_running_var']
        
        # Normalize input using batch statistics
        x_normalized = (x - sample_mean.reshape(1, C, 1, 1)) / np.sqrt(sample_var.reshape(1, C, 1, 1) + self.eps)
        
        # Apply scale and shift to normalized input
        out = self.params[self.name + '_gamma'].reshape(1, C, 1, 1) * x_normalized + self.params[self.name + '_beta'].reshape(1, C, 1, 1)
        
        # Save meta information for backpropagation
        self.meta = (x, x_normalized, sample_mean, sample_var)
        
        return out
    
    def backward(self, dout):
        x, x_normalized, sample_mean, sample_var = self.meta
        N, C, H, W = x.shape
        
        # Compute dx_normalized, dx, dgamma, and dbeta
        dx_normalized = dout * self.params[self.name + '_gamma'].reshape(1, C, 1, 1)
        dgamma = np.sum(dout * x_normalized, axis=(0, 2, 3))
        dbeta = np.sum(dout, axis=(0, 2, 3))
        
        dsample_var = np.sum(dx_normalized * (x - sample_mean.reshape(1, C, 1, 1)) * (-0.5) * (sample_var.reshape(1, C, 1, 1) + self.eps)**(-1.5), axis=(0, 2, 3))
        dsample_mean = np.sum(dx_normalized * (-1 / np.sqrt(sample_var.reshape(1, C, 1, 1) + self.eps)), axis=(0, 2, 3)) + dsample_var * np.mean(-2 * (x - sample_mean.reshape(1, C, 1, 1)), axis=(0, 2, 3))
        
        dx = dx_normalized / np.sqrt(sample_var.reshape(1, C, 1, 1) + self.eps) + dsample_var * 2 * (x - sample_mean.reshape(1, C, 1, 1)) / (N * H * W) + dsample_mean / (N * H * W)
        
        # Store gradients
        self.grads[self.name + '_gamma'] = dgamma
        self.grads[self.name + '_beta'] = dbeta
        
        return dx



class TestCNN_BN(Module):
    def __init__(self, dtype=np.float32, seed=None):
        self.net = sequential(
            ########## TODO: ##########
            ConvLayer2D(input_channels=3, kernel_size=3, number_filters=6, stride=1, padding=0, name='conv1'),
            BatchNorm2D(num_features=6, name='bn1'),
            MaxPoolingLayer(pool_size=2, stride=2, name='maxpool1'),
            flatten(name='flatten'),
            fc(input_dim=27, output_dim=5, init_scale=0.02, name='fc1'),
            ########### END ###########
        )


%reload_ext autoreload

seed = 1234
np.random.seed(seed=seed)

model = TestCNN_BN()
loss_func = cross_entropy()

B, H, W, iC = 4, 8, 8, 3 #batch, height, width, in_channels
k = 3 #kernel size
oC, Hi, O = 3, 27, 5 # out channels, Hidden Layer input, Output size
std = 0.02
x = np.random.randn(B,H,W,iC)
y = np.random.randint(O, size=B)

print ("Testing initialization ... ")

###################################################
# TODO: param_name should be replaced accordingly  #
###################################################
w1_std = abs(model.net.get_params("conv1_w").std() - std)
b1 = model.net.get_params("conv1_b").std()
w2_std = abs(model.net.get_params("fc1_w").std() - std)
b2 = model.net.get_params("fc1_b").std()
gamma = model.net.get_params("bn1_gamma")
beta = model.net.get_params("bn1_beta")

mean = model.net.get_params("bn1_running_mean")
var = model.net.get_params("bn1_running_var")
###################################################
#                END OF YOUR CODE                 #
###################################################

assert w1_std < std / 10, "First layer weights do not seem right"
assert np.all(b1 == 0), "First layer biases do not seem right"
assert w2_std < std / 10, "Second layer weights do not seem right"
assert np.all(b2 == 0), "Second layer biases do not seem right"
assert np.all(gamma == 1), "BatchNorm gamma does not seem right"
assert np.all(beta == 0), "BatchNorm beta does not seem right"
# assert np.all(mean == 0), "BatchNorm running mean does not seem right"
# assert np.all(var == 1), "BatchNorm running variance does not seem right"
print ("Passed!")

print ("Testing test-time forward pass ... ")
w1 = np.linspace(-0.7, 0.3, num=k*k*iC*oC).reshape(k,k,iC,oC)
w2 = np.linspace(-0.2, 0.2, num=Hi*O).reshape(Hi, O)
b1 = np.linspace(-0.6, 0.2, num=oC)
b2 = np.linspace(-0.9, 0.1, num=O)
gamma = np.ones(oC)
beta = np.zeros(oC)
mean = np.zeros(oC)
var = np.ones(oC)

###################################################
# TODO: param_name should be replaced accordingly  #
###################################################
model.net.assign("conv1_w", w1)
model.net.assign("conv1_b", b1)
model.net.assign("fc1_w", w2)
model.net.assign("fc1_b", b2)
model.net.assign("bn1_gamma", gamma)
model.net.assign("bn1_beta", beta)
model.net.assign("bn1_running_mean", mean)
model.net.assign("bn1_running_var", var)
###################################################
#                END OF YOUR CODE                 #
###################################################

feats = np.linspace(-5.5, 4.5, num=B*H*W*iC).reshape(B,H,W,iC)
scores = model.forward(feats)
correct_scores = np.asarray([[-13.84938115, -11.52013445,  -9.19088775,  -6.86164105,  -4.53239435],
 [-11.44301711, -10.21778439,  -8.99255167,  -7.76731895,  -6.54208622],
 [ -9.03665308,  -8.91543433,  -8.79421558,  -8.67299683,  -8.55177808],
 [ -6.63028904,  -7.61208428,  -8.59387952,  -9.57567476, -10.55747001]])
scores_diff = np.sum(np.abs(scores - correct_scores))
assert scores_diff < 1e-6, "Your implementation might be wrong!"
print ("Passed!")

print ("Testing the loss ...",)
y = np.asarray([0, 2, 1, 4])
loss = loss_func.forward(scores, y)
dLoss = loss_func.backward()
correct_loss = 4.532452514565059
assert abs(loss - correct_loss) < 1e-10, "Your implementation might be wrong!"
print ("Passed!")

print ("Testing the gradients (error should be no larger than 1e-6) ...")
din = model.backward(dLoss)
for layer in model.net.layers:
    if not layer.params:
        continue
    for name in sorted(layer.grads):
        f = lambda _: loss_func.forward(model.forward(feats), y)
        grad_num = eval_numerical_gradient(f, layer.params[name], verbose=False)
        print ('%s relative error: %.2e' % (name, rel_error(grad_num, layer.grads[name])))